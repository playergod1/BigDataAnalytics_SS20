{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 1.2: A simple parallel MapReduce framework in Python\n",
    "Now we use the [*multiprocessing*](https://docs.python.org/3/library/multiprocessing.html) lib to actually do things in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "class SimpleMapReduce:\n",
    "\n",
    "    def __init__(self, map_func, reduce_func, num_workers=None):\n",
    "        \"\"\"\n",
    "        map_func\n",
    "\n",
    "          Function to map inputs to intermediate data. Takes as\n",
    "          argument one input value and returns a tuple with the\n",
    "          key and a value to be reduced.\n",
    "\n",
    "        reduce_func\n",
    "\n",
    "          Function to reduce partitioned version of intermediate\n",
    "          data to final output. Takes as argument a key as\n",
    "          produced by map_func and a sequence of the values\n",
    "          associated with that key.\n",
    "\n",
    "        num_workers\n",
    "\n",
    "          The number of workers to create in the pool. Defaults\n",
    "          to the number of CPUs available on the current host.\n",
    "        \"\"\"\n",
    "        self.map_func = map_func\n",
    "        self.reduce_func = reduce_func\n",
    "        self.pool = multiprocessing.Pool(num_workers)\n",
    "\n",
    "    def partition(self, mapped_values):\n",
    "        \"\"\"Organize the mapped values by their key.\n",
    "        Returns an unsorted sequence of tuples with a key\n",
    "        and a sequence of values.\n",
    "        \"\"\"\n",
    "        partitioned_data = collections.defaultdict(list)\n",
    "        for key, value in mapped_values:\n",
    "            partitioned_data[key].append(value)\n",
    "        return partitioned_data.items()\n",
    "\n",
    "    def __call__(self, inputs, chunksize=1):\n",
    "        \"\"\"Process the inputs through the map and reduce functions\n",
    "        given.\n",
    "\n",
    "        inputs\n",
    "          An iterable containing the input data to be processed.\n",
    "\n",
    "        chunksize=1\n",
    "          The portion of the input data to hand to each worker.\n",
    "          This can be used to tune performance during the mapping\n",
    "          phase.\n",
    "        \"\"\"\n",
    "        map_responses = self.pool.map(\n",
    "            self.map_func,\n",
    "            inputs,\n",
    "            chunksize=chunksize,\n",
    "        )\n",
    "        partitioned_data = self.partition(\n",
    "            itertools.chain(*map_responses)\n",
    "        )\n",
    "        reduced_values = self.pool.map(\n",
    "            self.reduce_func,\n",
    "            partitioned_data,\n",
    "        )\n",
    "        return reduced_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Word Count example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# map - read file and count words\n",
    "def file_to_words(filename):\n",
    "    \"\"\"Read a file and return a sequence of\n",
    "    (word, occurences) values.\n",
    "    \"\"\"\n",
    "    STOP_WORDS = set([\n",
    "        'a', 'an', 'and', 'are', 'as', 'be', 'by', 'for', 'if',\n",
    "        'in', 'is', 'it', 'of', 'or', 'py', 'rst', 'that', 'the',\n",
    "        'to', 'with',\n",
    "    ])\n",
    "    TR = str.maketrans({\n",
    "        p: ' '\n",
    "        for p in string.punctuation\n",
    "    })\n",
    "\n",
    "    print('{} reading {}'.format(\n",
    "        multiprocessing.current_process().name, filename))\n",
    "    output = []\n",
    "\n",
    "    with open(filename, 'rt') as f:\n",
    "        for line in f:\n",
    "            # Skip comment lines.\n",
    "            if line.lstrip().startswith('..'):\n",
    "                continue\n",
    "            line = line.translate(TR)  # Strip punctuation\n",
    "            for word in line.split():\n",
    "                word = word.lower()\n",
    "                if word.isalpha() and word not in STOP_WORDS:\n",
    "                    output.append((word, 1))\n",
    "    return output\n",
    "\n",
    "# reduce\n",
    "def count_words(item):\n",
    "    \"\"\"Convert the partitioned data for a word to a\n",
    "    tuple containing the word and the number of occurences.\n",
    "    \"\"\"\n",
    "    word, occurences = item\n",
    "    return (word, sum(occurences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import glob #easy file access\n",
    "\n",
    "input_files = glob.glob('*.rst')#get all text files names \n",
    "\n",
    "mapper = SimpleMapReduce(file_to_words, count_words)#get mapreduce instance with custom map and reduce functions\n",
    "word_counts = mapper(input_files)#call parallel mapreduce on files\n",
    "\n",
    "word_counts.sort(key=operator.itemgetter(1))#sort results\n",
    "word_counts.reverse()\n",
    "\n",
    "print('\\nTOP 20 WORDS BY FREQUENCY\\n')\n",
    "top20 = word_counts[:20]\n",
    "longest = max(len(word) for word, count in top20)\n",
    "for word, count in top20:\n",
    "    print('{word:<{len}}: {count:5}'.format(\n",
    "            len=longest + 1,\n",
    "            word=word,\n",
    "            count=count)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means implementation\n",
    "Use the simple frame work to implement K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate data with K=4\n",
    "X, y = make_blobs(n_samples=1500, centers=4)\n",
    "plt.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means ref implementation\n",
    "k = KMeans(n_clusters=4, random_state=42)\n",
    "y_pred = k.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement map -> assign data to center \n",
    "def kMap( ... ): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement reduce -> compute new centers\n",
    "def kReduce( ... ):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init center [[-7.43559786 -2.53818871]\n",
      " [-9.68259007 -2.75656783]\n",
      " [-9.92905839 -2.02917772]\n",
      " [ 5.88128798  8.58245137]]\n"
     ]
    }
   ],
   "source": [
    "#init centers\n",
    "center = X[np.random.randint(X.shape[0], size=4), :]#get 4 random datapoints\n",
    "print('init center', center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kMap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2df835bb7630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#use framework\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleMapReduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkMap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkReduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#get mapreduce instance with custom map and reduce functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcenter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#call parallel mapreduce oo data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kMap' is not defined"
     ]
    }
   ],
   "source": [
    "#use framework\n",
    "mapper = SimpleMapReduce(kMap, kReduce)#get mapreduce instance with custom map and reduce functions\n",
    "center = mapper(X)#call parallel mapreduce oo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
